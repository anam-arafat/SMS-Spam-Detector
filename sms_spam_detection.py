# -*- coding: utf-8 -*-
"""SMS Spam Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yqphnO2gsjwXdXf9IWcE1JH4NgYJcgkB

##Loading Dataset
"""

# Import libraries & upload dataset
import numpy as np
import pandas as pd
from google.colab import files
uploaded = files.upload()

# Load the dataset and display 5 random sample rows
df = pd.read_csv('spam.csv', encoding='latin-1')
df.sample(5)

# Total rows and columns of dataset
df.shape

"""##Data Cleaning"""

# Check if unnamed columns contain data
df.info()

# Drop columns with minimal data
df.drop(columns = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace = True) # Drop specified columns permanently (inplace=True)
df.sample(5)

# Renaming the columns
df.rename(columns = {'v1' : 'target', 'v2' : 'text'}, inplace = True)
df.sample(5)

# Initialize LabelEncoder for encoding categorical labels
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

# Convert 'target' column values to numeric (0s and 1s) using LabelEncoder
df['target'] = encoder.fit_transform(df['target'])
df.head()

# Checking for missing values in cells
df.isnull().sum()

# Check for duplicate values
df.duplicated().sum()

# Remove the duplicates
df = df.drop_duplicates(keep = 'first')
df.duplicated().sum()

df.shape

"""## Exploratory Data Analysis (EDA)"""

# Count and display the number of occurrences for each value in the 'target' column
df['target'].value_counts()

# Plot a pie chart to visualize the distribution of 'target' values
import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(), labels = ['ham', 'spam'], autopct = '%0.2f')
plt.show()

# Data is imbalanced

# Install and download NLTK resources for text analysis
!pip install nltk
import nltk
nltk.download('punkt')

# Add a new column with the number of characters in each 'text' entry
df['num_characters'] = df['text'].apply(len)
df.head()

# Add a new column with the number of words in each 'text' entry
df['num_words'] = df['text'].apply(lambda x : len(nltk.word_tokenize(x)))
df.head()

# Add a new column with the number of sentences in each 'text' entry
df['num_sentences'] = df['text'].apply(lambda x : len(nltk.sent_tokenize(x)))
df.head()

# Show descriptive statistics for 'num_characters', 'num_words', and 'num_sentences' columns
df[['num_characters', 'num_words', 'num_sentences']].describe()

# Show descriptive statistics for 'num_characters', 'num_words', and 'num_sentences' for 'Ham' messages
df[df['target'] == 0][['num_characters', 'num_words', 'num_sentences']].describe()

# Show descriptive statistics for 'num_characters', 'num_words', and 'num_sentences' for 'Spam' messages
df[df['target'] == 1][['num_characters', 'num_words', 'num_sentences']].describe()

# Plot histograms of 'num_characters' for 'Ham' and 'Spam' messages
import seaborn as sns
plt.figure(figsize = (12, 6))
sns.histplot(df[df['target'] == 0]['num_characters'])
sns.histplot(df[df['target'] == 1]['num_characters'], color = 'red')

# Plot histograms of 'num_words' for 'Ham' and 'Spam' messages
plt.figure(figsize = (12, 6))
sns.histplot(df[df['target'] == 0]['num_words'])
sns.histplot(df[df['target'] == 1]['num_words'], color = 'red')

# Plot histograms of 'num_sentences' for 'Ham' and 'Spam' messages
plt.figure(figsize = (12, 6))
sns.histplot(df[df['target'] == 0]['num_sentences'])
sns.histplot(df[df['target'] == 1]['num_sentences'], color = 'red')

# Plot pairwise relationships and correlations between columns, colored by 'target'
sns.pairplot(df, hue = 'target')

# Convert 'target' column to numeric, then calculate and plot the correlation heatmap for numerical columns
df['target'] = pd.to_numeric(df['target'])
numerical_df = df.select_dtypes(include=['number'])
sns.heatmap(numerical_df.corr(), annot = True)

"""##Data Preprocessing
- Lowercase
- Tokenization
- Removing special characters
- Removing stop words and punctuations
- Stemming
"""

# Download stopwords, import them, and display punctuation and English stopwords
nltk.download('stopwords')
from nltk.corpus import stopwords
import string
string.punctuation
stopwords.words('english')

# Function to preprocess text: convert to lowercase, tokenize, remove special characters, stopwords, and punctuation, and apply stemming
def transform_text(text):
  text = text.lower() # Lowercase

  text = nltk.word_tokenize(text) # Tokenization

  y = []
  for i in text: # Removing special characters
    if i.isalnum():
      y.append(i)

  text = y[:]
  y.clear()
  for i in text: # Removing stop words and puhctuations
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)

  from nltk.stem.porter import PorterStemmer
  ps = PorterStemmer()
  z = []
  for i in y: # Stemming
    z.append(ps.stem(i))

  return " ".join(z)

# Apply text transformation function to the 'text' column and store results in 'transformed_text'
df['transformed_text'] = df['text'].apply(transform_text)
df.head()

# Initialize WordCloud with specified width, height, font size, and background color
from wordcloud import WordCloud
wc = WordCloud(width = 500, height = 500, min_font_size = 10, background_color = 'white')

# Generate and display a word cloud for the most common words in spam messages
spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep = " "))
plt.figure(figsize = (15, 7))
plt.imshow(spam_wc)

# Generate and display a word cloud for the most common words in ham messages
ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep = " "))
plt.figure(figsize = (15, 7))
plt.imshow(spam_wc)

# Create a bar plot of the 30 most common words in spam messages
from collections import Counter
spam_corpus = []
for msg in df[df['target'] == 1]['transformed_text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)
spam_df = pd.DataFrame(Counter(spam_corpus).most_common(30))
sns.barplot(x=spam_df[0], y=spam_df[1]) # Use the 'x' and 'y' keywords to specify the columns
plt.xticks(rotation='vertical')
plt.show()

# Create a bar plot of the 30 most common words in ham messages
ham_corpus = []
for msg in df[df['target'] == 0]['transformed_text'].tolist():
  for word in msg.split():
    ham_corpus.append(word)
spam_df = pd.DataFrame(Counter(ham_corpus).most_common(30))
sns.barplot(x=spam_df[0], y=spam_df[1]) # Use the 'x' and 'y' keywords to specify the columns
plt.xticks(rotation='vertical')
plt.show()

"""##Model Building"""

# Import and initialize CountVectorizer and TfidfVectorizer for text feature extraction
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
cv = CountVectorizer()
tfidf = TfidfVectorizer(max_features = 3000)

# Convert 'transformed_text' to a matrix of token counts using TF-IDF for better precision
    # X = cd.fit_transform(df['transformed_text']).toarray() -> Has less precision
    # from sklearn.preprocessing import MinMaxScaler  -> Has less precision
      # scaler = MinMaxScaler()
      # X = scaler.fit_transform(X)
X = tfidf.fit_transform(df['transformed_text']).toarray()

# Extract the 'target' column from the dataframe as a NumPy array
y = df['target'].values
y

# Split data into 80% train and 20% test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)

# Import Naive Bayes classifiers and metrics
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score
gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

# Train Gaussian Naive Bayes, make predictions, and print accuracy, confusion matrix, and precision
gnb.fit(X_train, y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test, y_pred1))
print(confusion_matrix(y_test, y_pred1))
print(precision_score(y_test, y_pred1))

# Train Multinomial Naive Bayes, make predictions, and print accuracy, confusion matrix, and precision
mnb.fit(X_train, y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test, y_pred2))
print(confusion_matrix(y_test, y_pred2))
print(precision_score(y_test, y_pred2))

# Train Bernoulli Naive Bayes, make predictions, and print accuracy, confusion matrix, and precision
bnb.fit(X_train, y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test, y_pred3))
print(confusion_matrix(y_test, y_pred3))
print(precision_score(y_test, y_pred3))

# Import various ML models for comparison, with Multinomial Naive Bayes (MNB) as the best precision using TF-IDF
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

# Store initialized classifiers in a dictionary for easier model comparison
svc = SVC(kernel = 'sigmoid', gamma = 1.0)
knc = KNeighborsClassifier()
mnb = MultinomialNB()
dtc = DecisionTreeClassifier(max_depth = 5)
lrc = LogisticRegression(solver = 'liblinear', penalty = 'l1')
rfc = RandomForestClassifier(n_estimators = 50, random_state = 2)
abc = AdaBoostClassifier(n_estimators = 50, random_state = 2)
bc = BaggingClassifier(n_estimators = 50, random_state = 2)
etc = ExtraTreesClassifier(n_estimators = 50, random_state = 2)
gbdt = GradientBoostingClassifier(n_estimators = 50, random_state = 2)
xgb = XGBClassifier(n_estimators = 50, random_state = 2)
clfs = {
    'SVC' : svc,
    'KN' : knc,
    'NB' : mnb,
    'DT' : dtc,
    'LR' : lrc,
    'RF' : rfc,
    'AdaBoost' : abc,
    'BgC' : bc,
    'ETC' : etc,
    'GBDT' : gbdt,
    'xgb' : xgb
}

# Function to train a classifier, make predictions, and return accuracy and precision
def train_classifier(clf, X_train, y_train, X_test, y_test):
  clf.fit(X_train, y_train)
  y_pred = clf.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred)
  return accuracy, precision

# Train the SVC model and return accuracy and precision on the test data
train_classifier(svc, X_train, y_train, X_test, y_test)

# Train each classifier, print its accuracy and precision, and store the results in lists
accuracy_scores = []
precision_scores = []
for name, clf in clfs.items():
  current_accuracy, current_precision = train_classifier(clf, X_train, y_train, X_test, y_test)
  print(f'{name} accuracy is {current_accuracy}')
  print(f'{name} precision is {current_precision}')
  accuracy_scores.append(current_accuracy)
  precision_scores.append(current_precision)

# Create a DataFrame to display the accuracy and precision of each algorithm, sorted by precision
performance_df = pd.DataFrame({'Algorithm' : clfs.keys(), 'Accuracy' : accuracy_scores, 'Precision' : precision_scores}).sort_values('Precision', ascending = False)
performance_df

# Plot a bar chart to compare accuracy and precision of each algorithm, with values between 0.5 and 1.0
sns.catplot(x = 'Algorithm', y = 'value', hue = 'variable', data = performance_df.melt(id_vars = 'Algorithm'), kind = 'bar', height = 5)
plt.ylim(0.5, 1.0)
plt.xticks(rotation = 'vertical')
plt.show()

"""##Improve Model"""

# Merge performance data with results after changing TF-IDF max_features and scaling, comparing different metrics
temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)
temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_scaling':accuracy_scores,'Precision_scaling':precision_scores}).sort_values('Precision_scaling',ascending=False)
new_df = performance_df.merge(temp_df,on='Algorithm')
new_df_scaled = new_df.merge(temp_df,on='Algorithm')
temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_num_chars':accuracy_scores,'Precision_num_chars':precision_scores}).sort_values('Precision_num_chars',ascending=False)
new_df_scaled.merge(temp_df,on='Algorithm')

# Initialize and train a soft voting classifier with SVC, MultinomialNB, and ExtraTreesClassifier
svc = SVC(kernel='sigmoid', gamma=1.0,probability=True)
mnb = MultinomialNB()
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
from sklearn.ensemble import VotingClassifier
voting = VotingClassifier(estimators=[('svm', svc), ('nb', mnb), ('et', etc)],voting='soft')
voting.fit(X_train,y_train)

# Predict with the voting classifier and print the accuracy and precision on the test data
y_pred = voting.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred))

# Initialize base estimators and a final estimator for stacking
estimators=[('svm', svc), ('nb', mnb), ('et', etc)]
final_estimator = RandomForestClassifier()

# Train a stacking classifier with base estimators and a final estimator, and print accuracy and precision
from sklearn.ensemble import StackingClassifier
clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred))

# Save the TF-IDF vectorizer and the MultinomialNB model to disk using pickle
import pickle
pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(mnb,open('model.pkl','wb'))

"""##Running the model"""

import ipywidgets as widgets
from IPython.display import display
import pickle
import nltk
import string
from nltk.corpus import stopwords

# Ensure nltk resources are downloaded
nltk.download('stopwords')
nltk.download('punkt')

# Load the saved model and vectorizer
try:
    tfidf = pickle.load(open('vectorizer.pkl', 'rb'))
    model = pickle.load(open('model.pkl', 'rb'))
    print("Model and vectorizer loaded successfully.")
except Exception as e:
    print(f"Error loading model or vectorizer: {e}")

# Define text transformation function
def transform_text(text):
    text = text.lower()  # Lowercase
    text = nltk.word_tokenize(text)  # Tokenization
    text = [i for i in text if i.isalnum()]  # Remove special characters
    text = [i for i in text if i not in stopwords.words('english') and i not in string.punctuation]  # Remove stopwords and punctuation
    from nltk.stem.porter import PorterStemmer
    ps = PorterStemmer()
    text = [ps.stem(i) for i in text]  # Stemming
    return " ".join(text)

# Function to predict spam or ham
def predict_spam(text):
    transformed_text = transform_text(text)
    text_vector = tfidf.transform([transformed_text]).toarray()
    prediction = model.predict(text_vector)
    return "Spam" if prediction[0] == 1 else "Not Spam"

# Create a textbox widget
text_box = widgets.Text(
    description='Enter Message:',
    placeholder='Type your message here'
)

# Create a button widget
button = widgets.Button(description="Check for Spam")

# Create an output widget to display the result
output = widgets.Output()

# Define the function to be called when the button is clicked
def on_button_clicked(b):
    with output:
        # Clear previous output
        output.clear_output()
        # Get the text from the textbox
        message = text_box.value
        if message:  # Check if the message is not empty
            # Get prediction
            result = predict_spam(message)
            # Display the result
            print(f'The message is classified as: {result}')
        else:
            print("Please enter a message to check.")

# Link the button click event to the function
button.on_click(on_button_clicked)

# Display the widgets
display(text_box, button, output)